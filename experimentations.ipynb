{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from langsmith import Client\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.smith import RunEvalConfig, run_on_dataset\n",
    "\n",
    "# To Avoid the Error on Jupyter Notebook (RuntimeError: This Event Loop Is Already Running)\n",
    "# Patch Asyncio To Allow Nested Event Loops\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Haide\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `predict` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to batch ingest runs: LangSmithError('Failed to post https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to post https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n"
     ]
    }
   ],
   "source": [
    "# load_dotenv(find_dotenv())\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = str(os.getenv(\"Langchain_API\"))\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"ATS\"\n",
    "\n",
    "client = Client()\n",
    "\n",
    "# llm = ChatOpenAI()\n",
    "llm.predict(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "[Errno 403 Client Error: Forbidden for url: https://api.smith.langchain.com/datasets] {\"detail\":\"Forbidden\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Haide\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langsmith\\utils.py:102\u001b[0m, in \u001b[0;36mraise_for_status_with_text\u001b[1;34m(response)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 102\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Haide\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://api.smith.langchain.com/datasets",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m dataset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRap Battle Dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Storing inputs in a dataset lets us\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# run chains and LLMs over a shared set of examples.\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRap battle prompts.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_prompt \u001b[38;5;129;01min\u001b[39;00m example_inputs:\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# Each example must be unique and have inputs defined.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# Outputs are optional\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     client\u001b[38;5;241m.\u001b[39mcreate_example(\n\u001b[0;32m     21\u001b[0m         inputs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_prompt},\n\u001b[0;32m     22\u001b[0m         outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     23\u001b[0m         dataset_id\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mid,\n\u001b[0;32m     24\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Haide\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langsmith\\client.py:2201\u001b[0m, in \u001b[0;36mClient.create_dataset\u001b[1;34m(self, dataset_name, description, data_type)\u001b[0m\n\u001b[0;32m   2191\u001b[0m dataset \u001b[38;5;241m=\u001b[39m ls_schemas\u001b[38;5;241m.\u001b[39mDatasetCreate(\n\u001b[0;32m   2192\u001b[0m     name\u001b[38;5;241m=\u001b[39mdataset_name,\n\u001b[0;32m   2193\u001b[0m     description\u001b[38;5;241m=\u001b[39mdescription,\n\u001b[0;32m   2194\u001b[0m     data_type\u001b[38;5;241m=\u001b[39mdata_type,\n\u001b[0;32m   2195\u001b[0m )\n\u001b[0;32m   2196\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mpost(\n\u001b[0;32m   2197\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_url \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/datasets\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2198\u001b[0m     headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_headers, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m   2199\u001b[0m     data\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mjson(),\n\u001b[0;32m   2200\u001b[0m )\n\u001b[1;32m-> 2201\u001b[0m \u001b[43mls_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status_with_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ls_schemas\u001b[38;5;241m.\u001b[39mDataset(\n\u001b[0;32m   2203\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse\u001b[38;5;241m.\u001b[39mjson(),\n\u001b[0;32m   2204\u001b[0m     _host_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_host_url,\n\u001b[0;32m   2205\u001b[0m     _tenant_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_optional_tenant_id(),\n\u001b[0;32m   2206\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Haide\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langsmith\\utils.py:104\u001b[0m, in \u001b[0;36mraise_for_status_with_text\u001b[1;34m(response)\u001b[0m\n\u001b[0;32m    102\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mHTTPError(\u001b[38;5;28mstr\u001b[39m(e), response\u001b[38;5;241m.\u001b[39mtext) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mHTTPError\u001b[0m: [Errno 403 Client Error: Forbidden for url: https://api.smith.langchain.com/datasets] {\"detail\":\"Forbidden\"}"
     ]
    }
   ],
   "source": [
    "example_inputs = [\n",
    "    \"a rap battle between Atticus Finch and Cicero\",\n",
    "    \"a rap battle between Barbie and Oppenheimer\",\n",
    "    \"a Pythonic rap battle between two swallows: one European and one African\",\n",
    "    \"a rap battle between Aubrey Plaza and Stephen Colbert\",\n",
    "]\n",
    "\n",
    "dataset_name = \"Rap Battle Dataset\"\n",
    "\n",
    "# Storing inputs in a dataset lets us\n",
    "# run chains and LLMs over a shared set of examples.\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"Rap battle prompts.\",\n",
    ")\n",
    "\n",
    "for input_prompt in example_inputs:\n",
    "    # Each example must be unique and have inputs defined.\n",
    "    # Outputs are optional\n",
    "    client.create_example(\n",
    "        inputs={\"question\": input_prompt},\n",
    "        outputs=None,\n",
    "        dataset_id=dataset.id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "LangSmithError",
     "evalue": "Failed to get https://api.smith.langchain.com/datasets in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/datasets?limit=1&name=System+Prompts', '{\"detail\":\"Forbidden\"}')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Haide\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langsmith\\utils.py:102\u001b[0m, in \u001b[0;36mraise_for_status_with_text\u001b[1;34m(response)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 102\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Haide\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://api.smith.langchain.com/datasets?limit=1&name=System+Prompts",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Haide\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langsmith\\client.py:674\u001b[0m, in \u001b[0;36mrequest_with_retries\u001b[1;34m(self, request_method, url, request_kwargs, stop_after_attempt, retry_on, to_ignore, handle_response)\u001b[0m\n\u001b[0;32m    672\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\n\u001b[1;32m--> 674\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest_with_retries\u001b[39m(\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;241m/\u001b[39m,\n\u001b[0;32m    677\u001b[0m     method: Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPUT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPATCH\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDELETE\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    678\u001b[0m     pathname: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m    680\u001b[0m     request_kwargs: Optional[Mapping] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    681\u001b[0m     stop_after_attempt: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    682\u001b[0m     retry_on: Optional[Sequence[Type[\u001b[38;5;167;01mBaseException\u001b[39;00m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    683\u001b[0m     to_ignore: Optional[Sequence[Type[\u001b[38;5;167;01mBaseException\u001b[39;00m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    684\u001b[0m     handle_response: Optional[Callable[[requests\u001b[38;5;241m.\u001b[39mResponse, \u001b[38;5;28mint\u001b[39m], Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    685\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    686\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m requests\u001b[38;5;241m.\u001b[39mResponse:\n\u001b[0;32m    687\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Send a request with retries.\u001b[39;00m\n\u001b[0;32m    688\u001b[0m \n\u001b[0;32m    689\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;124;03m        If the request fails.\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Haide\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langsmith\\utils.py:104\u001b[0m, in \u001b[0;36mraise_for_status_with_text\u001b[1;34m(response)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mHTTPError(\u001b[38;5;28mstr\u001b[39m(e), response\u001b[38;5;241m.\u001b[39mtext) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mHTTPError\u001b[0m: [Errno 403 Client Error: Forbidden for url: https://api.smith.langchain.com/datasets?limit=1&name=System+Prompts] {\"detail\":\"Forbidden\"}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLangSmithError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 27\u001b[0m\n\u001b[0;32m     17\u001b[0m eval_config \u001b[38;5;241m=\u001b[39m smith\u001b[38;5;241m.\u001b[39mRunEvalConfig(\n\u001b[0;32m     18\u001b[0m     evaluators\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     19\u001b[0m         smith\u001b[38;5;241m.\u001b[39mRunEvalConfig\u001b[38;5;241m.\u001b[39mCriteria(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhelpfulness\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m     eval_llm\u001b[38;5;241m=\u001b[39mchat_models\u001b[38;5;241m.\u001b[39mChatOpenAI(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4\u001b[39m\u001b[38;5;124m\"\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msk-AHwDWlMFWnKw2byQmcdwT3BlbkFJH6dVhCqLDtFQuVz65fzI\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m )\n\u001b[0;32m     26\u001b[0m client \u001b[38;5;241m=\u001b[39m langsmith\u001b[38;5;241m.\u001b[39mClient()\n\u001b[1;32m---> 27\u001b[0m chain_results \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_on_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSystem Prompts\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm_or_chain_factory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mATS resume and job matching\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconcurrency_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Haide\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langsmith\\client.py:4067\u001b[0m, in \u001b[0;36mrun_on_dataset\u001b[1;34m(self, dataset_name, llm_or_chain_factory, evaluation, concurrency_level, project_name, project_metadata, dataset_version, verbose, input_mapper, revision_id, **kwargs)\u001b[0m\n\u001b[0;32m   3975\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21marun_on_dataset\u001b[39m(\n\u001b[0;32m   3976\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3977\u001b[0m     dataset_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3988\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   3989\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m   3990\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Asynchronously run the Chain or language model on a dataset.\u001b[39;00m\n\u001b[0;32m   3991\u001b[0m \n\u001b[0;32m   3992\u001b[0m \u001b[38;5;124;03m    Store traces to the specified project name.\u001b[39;00m\n\u001b[0;32m   3993\u001b[0m \n\u001b[0;32m   3994\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   3995\u001b[0m \u001b[38;5;124;03m        dataset_name: Name of the dataset to run the chain on.\u001b[39;00m\n\u001b[0;32m   3996\u001b[0m \u001b[38;5;124;03m        llm_or_chain_factory: Language model or Chain constructor to run\u001b[39;00m\n\u001b[0;32m   3997\u001b[0m \u001b[38;5;124;03m            over the dataset. The Chain constructor is used to permit\u001b[39;00m\n\u001b[0;32m   3998\u001b[0m \u001b[38;5;124;03m            independent calls on each example without carrying over state.\u001b[39;00m\n\u001b[0;32m   3999\u001b[0m \u001b[38;5;124;03m        evaluation: Optional evaluation configuration to use when evaluating\u001b[39;00m\n\u001b[0;32m   4000\u001b[0m \u001b[38;5;124;03m        concurrency_level: The number of async tasks to run concurrently.\u001b[39;00m\n\u001b[0;32m   4001\u001b[0m \u001b[38;5;124;03m        project_name: Name of the project to store the traces in.\u001b[39;00m\n\u001b[0;32m   4002\u001b[0m \u001b[38;5;124;03m            Defaults to a randomly generated name.\u001b[39;00m\n\u001b[0;32m   4003\u001b[0m \u001b[38;5;124;03m        project_metadata: Optional metadata to store with the project.\u001b[39;00m\n\u001b[0;32m   4004\u001b[0m \u001b[38;5;124;03m        dataset_version: Optional version identifier to run the dataset on.\u001b[39;00m\n\u001b[0;32m   4005\u001b[0m \u001b[38;5;124;03m            Can be a timestamp or a string tag.\u001b[39;00m\n\u001b[0;32m   4006\u001b[0m \u001b[38;5;124;03m        verbose: Whether to print progress.\u001b[39;00m\n\u001b[0;32m   4007\u001b[0m \u001b[38;5;124;03m        tags: Tags to add to each run in the project.\u001b[39;00m\n\u001b[0;32m   4008\u001b[0m \u001b[38;5;124;03m        input_mapper: A function to map to the inputs dictionary from an Example\u001b[39;00m\n\u001b[0;32m   4009\u001b[0m \u001b[38;5;124;03m            to the format expected by the model to be evaluated. This is useful if\u001b[39;00m\n\u001b[0;32m   4010\u001b[0m \u001b[38;5;124;03m            your model needs to deserialize more complex schema or if your dataset\u001b[39;00m\n\u001b[0;32m   4011\u001b[0m \u001b[38;5;124;03m            has inputs with keys that differ from what is expected by your chain\u001b[39;00m\n\u001b[0;32m   4012\u001b[0m \u001b[38;5;124;03m            or agent.\u001b[39;00m\n\u001b[0;32m   4013\u001b[0m \u001b[38;5;124;03m        revision_id: Optional revision identifier to assign this test run to\u001b[39;00m\n\u001b[0;32m   4014\u001b[0m \u001b[38;5;124;03m            track the performance of different versions of your system.\u001b[39;00m\n\u001b[0;32m   4015\u001b[0m \n\u001b[0;32m   4016\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[0;32m   4017\u001b[0m \u001b[38;5;124;03m        A dictionary containing the run's project name and the\u001b[39;00m\n\u001b[0;32m   4018\u001b[0m \u001b[38;5;124;03m        resulting model outputs.\u001b[39;00m\n\u001b[0;32m   4019\u001b[0m \n\u001b[0;32m   4020\u001b[0m \u001b[38;5;124;03m    For the synchronous version, see client.run_on_dataset.\u001b[39;00m\n\u001b[0;32m   4021\u001b[0m \n\u001b[0;32m   4022\u001b[0m \u001b[38;5;124;03m    Examples:\u001b[39;00m\n\u001b[0;32m   4023\u001b[0m \u001b[38;5;124;03m    --------\u001b[39;00m\n\u001b[0;32m   4024\u001b[0m \u001b[38;5;124;03m    .. code-block:: python\u001b[39;00m\n\u001b[0;32m   4025\u001b[0m \n\u001b[0;32m   4026\u001b[0m \u001b[38;5;124;03m        from langsmith import Client\u001b[39;00m\n\u001b[0;32m   4027\u001b[0m \u001b[38;5;124;03m        from langchain.chat_models import ChatOpenAI\u001b[39;00m\n\u001b[0;32m   4028\u001b[0m \u001b[38;5;124;03m        from langchain.chains import LLMChain\u001b[39;00m\n\u001b[0;32m   4029\u001b[0m \u001b[38;5;124;03m        from langchain.smith import RunEvalConfig\u001b[39;00m\n\u001b[0;32m   4030\u001b[0m \n\u001b[0;32m   4031\u001b[0m \n\u001b[0;32m   4032\u001b[0m \u001b[38;5;124;03m        # Chains may have memory. Passing in a constructor function lets the\u001b[39;00m\n\u001b[0;32m   4033\u001b[0m \u001b[38;5;124;03m        # evaluation framework avoid cross-contamination between runs.\u001b[39;00m\n\u001b[0;32m   4034\u001b[0m \u001b[38;5;124;03m        def construct_chain():\u001b[39;00m\n\u001b[0;32m   4035\u001b[0m \u001b[38;5;124;03m            llm = ChatOpenAI(temperature=0)\u001b[39;00m\n\u001b[0;32m   4036\u001b[0m \u001b[38;5;124;03m            chain = LLMChain.from_string(llm, \"What's the answer to {your_input_key}\")\u001b[39;00m\n\u001b[0;32m   4037\u001b[0m \u001b[38;5;124;03m            return chain\u001b[39;00m\n\u001b[0;32m   4038\u001b[0m \n\u001b[0;32m   4039\u001b[0m \n\u001b[0;32m   4040\u001b[0m \u001b[38;5;124;03m        # Load off-the-shelf evaluators via config or the EvaluatorType (string or enum)\u001b[39;00m\n\u001b[0;32m   4041\u001b[0m \u001b[38;5;124;03m        evaluation_config = RunEvalConfig(\u001b[39;00m\n\u001b[0;32m   4042\u001b[0m \u001b[38;5;124;03m            evaluators=[\u001b[39;00m\n\u001b[0;32m   4043\u001b[0m \u001b[38;5;124;03m                \"qa\",  # \"Correctness\" against a reference answer\u001b[39;00m\n\u001b[0;32m   4044\u001b[0m \u001b[38;5;124;03m                \"embedding_distance\",\u001b[39;00m\n\u001b[0;32m   4045\u001b[0m \u001b[38;5;124;03m                RunEvalConfig.Criteria(\"helpfulness\"),\u001b[39;00m\n\u001b[0;32m   4046\u001b[0m \u001b[38;5;124;03m                RunEvalConfig.Criteria(\u001b[39;00m\n\u001b[0;32m   4047\u001b[0m \u001b[38;5;124;03m                    {\u001b[39;00m\n\u001b[0;32m   4048\u001b[0m \u001b[38;5;124;03m                        \"fifth-grader-score\": \"Do you have to be smarter than a fifth grader to answer this question?\"\u001b[39;00m\n\u001b[0;32m   4049\u001b[0m \u001b[38;5;124;03m                    }\u001b[39;00m\n\u001b[0;32m   4050\u001b[0m \u001b[38;5;124;03m                ),\u001b[39;00m\n\u001b[0;32m   4051\u001b[0m \u001b[38;5;124;03m            ]\u001b[39;00m\n\u001b[0;32m   4052\u001b[0m \u001b[38;5;124;03m        )\u001b[39;00m\n\u001b[0;32m   4053\u001b[0m \n\u001b[0;32m   4054\u001b[0m \u001b[38;5;124;03m        client = Client()\u001b[39;00m\n\u001b[0;32m   4055\u001b[0m \u001b[38;5;124;03m        await client.arun_on_dataset(\u001b[39;00m\n\u001b[0;32m   4056\u001b[0m \u001b[38;5;124;03m            \"<my_dataset_name>\",\u001b[39;00m\n\u001b[0;32m   4057\u001b[0m \u001b[38;5;124;03m            construct_chain,\u001b[39;00m\n\u001b[0;32m   4058\u001b[0m \u001b[38;5;124;03m            evaluation=evaluation_config,\u001b[39;00m\n\u001b[0;32m   4059\u001b[0m \u001b[38;5;124;03m        )\u001b[39;00m\n\u001b[0;32m   4060\u001b[0m \n\u001b[0;32m   4061\u001b[0m \u001b[38;5;124;03m    You can also create custom evaluators by subclassing the\u001b[39;00m\n\u001b[0;32m   4062\u001b[0m \u001b[38;5;124;03m    :class:`StringEvaluator <langchain.evaluation.schema.StringEvaluator>`\u001b[39;00m\n\u001b[0;32m   4063\u001b[0m \u001b[38;5;124;03m    or LangSmith's `RunEvaluator` classes.\u001b[39;00m\n\u001b[0;32m   4064\u001b[0m \n\u001b[0;32m   4065\u001b[0m \u001b[38;5;124;03m    .. code-block:: python\u001b[39;00m\n\u001b[0;32m   4066\u001b[0m \n\u001b[1;32m-> 4067\u001b[0m \u001b[38;5;124;03m        from typing import Optional\u001b[39;00m\n\u001b[0;32m   4068\u001b[0m \u001b[38;5;124;03m        from langchain.evaluation import StringEvaluator\u001b[39;00m\n\u001b[0;32m   4069\u001b[0m \n\u001b[0;32m   4070\u001b[0m \n\u001b[0;32m   4071\u001b[0m \u001b[38;5;124;03m        class MyStringEvaluator(StringEvaluator):\u001b[39;00m\n\u001b[0;32m   4072\u001b[0m \u001b[38;5;124;03m            @property\u001b[39;00m\n\u001b[0;32m   4073\u001b[0m \u001b[38;5;124;03m            def requires_input(self) -> bool:\u001b[39;00m\n\u001b[0;32m   4074\u001b[0m \u001b[38;5;124;03m                return False\u001b[39;00m\n\u001b[0;32m   4075\u001b[0m \n\u001b[0;32m   4076\u001b[0m \u001b[38;5;124;03m            @property\u001b[39;00m\n\u001b[0;32m   4077\u001b[0m \u001b[38;5;124;03m            def requires_reference(self) -> bool:\u001b[39;00m\n\u001b[0;32m   4078\u001b[0m \u001b[38;5;124;03m                return True\u001b[39;00m\n\u001b[0;32m   4079\u001b[0m \n\u001b[0;32m   4080\u001b[0m \u001b[38;5;124;03m            @property\u001b[39;00m\n\u001b[0;32m   4081\u001b[0m \u001b[38;5;124;03m            def evaluation_name(self) -> str:\u001b[39;00m\n\u001b[0;32m   4082\u001b[0m \u001b[38;5;124;03m                return \"exact_match\"\u001b[39;00m\n\u001b[0;32m   4083\u001b[0m \n\u001b[0;32m   4084\u001b[0m \u001b[38;5;124;03m            def _evaluate_strings(\u001b[39;00m\n\u001b[0;32m   4085\u001b[0m \u001b[38;5;124;03m                self, prediction, reference=None, input=None, **kwargs\u001b[39;00m\n\u001b[0;32m   4086\u001b[0m \u001b[38;5;124;03m            ) -> dict:\u001b[39;00m\n\u001b[0;32m   4087\u001b[0m \u001b[38;5;124;03m                return {\"score\": prediction == reference}\u001b[39;00m\n\u001b[0;32m   4088\u001b[0m \n\u001b[0;32m   4089\u001b[0m \n\u001b[0;32m   4090\u001b[0m \u001b[38;5;124;03m        evaluation_config = RunEvalConfig(\u001b[39;00m\n\u001b[0;32m   4091\u001b[0m \u001b[38;5;124;03m            custom_evaluators=[MyStringEvaluator()],\u001b[39;00m\n\u001b[0;32m   4092\u001b[0m \u001b[38;5;124;03m        )\u001b[39;00m\n\u001b[0;32m   4093\u001b[0m \n\u001b[0;32m   4094\u001b[0m \u001b[38;5;124;03m        await client.arun_on_dataset(\u001b[39;00m\n\u001b[0;32m   4095\u001b[0m \u001b[38;5;124;03m            \"<my_dataset_name>\",\u001b[39;00m\n\u001b[0;32m   4096\u001b[0m \u001b[38;5;124;03m            construct_chain,\u001b[39;00m\n\u001b[0;32m   4097\u001b[0m \u001b[38;5;124;03m            evaluation=evaluation_config,\u001b[39;00m\n\u001b[0;32m   4098\u001b[0m \u001b[38;5;124;03m        )\u001b[39;00m\n\u001b[0;32m   4099\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m   4100\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   4101\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msmith\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m arun_on_dataset \u001b[38;5;28;01mas\u001b[39;00m _arun_on_dataset\n",
      "File \u001b[1;32mc:\\Users\\Haide\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\smith\\evaluation\\runner_utils.py:1370\u001b[0m, in \u001b[0;36mrun_on_dataset\u001b[1;34m(client, dataset_name, llm_or_chain_factory, evaluation, dataset_version, concurrency_level, project_name, project_metadata, verbose, revision_id, **kwargs)\u001b[0m\n\u001b[0;32m   1363\u001b[0m     revision_id \u001b[38;5;241m=\u001b[39m get_langchain_env_var_metadata()\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrevision_id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[0;32m   1366\u001b[0m     warn_deprecated(\n\u001b[0;32m   1367\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.0.305\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1368\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following arguments are deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1369\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in a future release: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1370\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1371\u001b[0m         removal\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.0.305\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1372\u001b[0m     )\n\u001b[0;32m   1373\u001b[0m client \u001b[38;5;241m=\u001b[39m client \u001b[38;5;129;01mor\u001b[39;00m Client()\n\u001b[0;32m   1374\u001b[0m container \u001b[38;5;241m=\u001b[39m _DatasetRunContainer\u001b[38;5;241m.\u001b[39mprepare(\n\u001b[0;32m   1375\u001b[0m     client,\n\u001b[0;32m   1376\u001b[0m     dataset_name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1385\u001b[0m     dataset_version\u001b[38;5;241m=\u001b[39mdataset_version,\n\u001b[0;32m   1386\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Haide\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\smith\\evaluation\\runner_utils.py:1174\u001b[0m, in \u001b[0;36mprepare\u001b[1;34m(cls, client, dataset_name, llm_or_chain_factory, project_name, evaluation, tags, input_mapper, concurrency_level, project_metadata, revision_id, dataset_version)\u001b[0m\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m   1159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare\u001b[39m(\n\u001b[0;32m   1160\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1171\u001b[0m     dataset_version: Optional[Union[datetime, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1172\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _DatasetRunContainer:\n\u001b[0;32m   1173\u001b[0m     project_name \u001b[38;5;241m=\u001b[39m project_name \u001b[38;5;129;01mor\u001b[39;00m name_generation\u001b[38;5;241m.\u001b[39mrandom_name()\n\u001b[1;32m-> 1174\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m revision_id:\n\u001b[0;32m   1175\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m project_metadata:\n\u001b[0;32m   1176\u001b[0m             project_metadata \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\Haide\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\smith\\evaluation\\runner_utils.py:973\u001b[0m, in \u001b[0;36m_prepare_eval_run\u001b[1;34m(client, dataset_name, llm_or_chain_factory, project_name, project_metadata, tags, dataset_version)\u001b[0m\n\u001b[0;32m    963\u001b[0m         result \u001b[38;5;241m=\u001b[39m EvalError(Error\u001b[38;5;241m=\u001b[39me)\n\u001b[0;32m    964\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m    967\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare_eval_run\u001b[39m(\n\u001b[0;32m    968\u001b[0m     client: Client,\n\u001b[0;32m    969\u001b[0m     dataset_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    970\u001b[0m     llm_or_chain_factory: MODEL_OR_CHAIN_FACTORY,\n\u001b[0;32m    971\u001b[0m     project_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    972\u001b[0m     project_metadata: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m--> 973\u001b[0m     tags: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    974\u001b[0m     dataset_version: Optional[Union[\u001b[38;5;28mstr\u001b[39m, datetime]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    975\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[MCF, TracerSession, Dataset, List[Example]]:\n\u001b[0;32m    976\u001b[0m     wrapped_model \u001b[38;5;241m=\u001b[39m _wrap_in_chain_factory(llm_or_chain_factory, dataset_name)\n\u001b[0;32m    977\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mread_dataset(dataset_name\u001b[38;5;241m=\u001b[39mdataset_name)\n",
      "File \u001b[1;32mc:\\Users\\Haide\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langsmith\\utils.py:92\u001b[0m, in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m     88\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate exactly one arg in each group is not None.\"\"\"\u001b[39;00m\n\u001b[0;32m     89\u001b[0m     counts \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     90\u001b[0m         \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m arg_group \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(arg) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     91\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m arg_group \u001b[38;5;129;01min\u001b[39;00m arg_groups\n\u001b[1;32m---> 92\u001b[0m     ]\n\u001b[0;32m     93\u001b[0m     invalid_groups \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i, count \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(counts) \u001b[38;5;28;01mif\u001b[39;00m count \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m invalid_groups:\n",
      "File \u001b[1;32mc:\\Users\\Haide\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langsmith\\client.py:2260\u001b[0m, in \u001b[0;36mread_dataset\u001b[1;34m(self, dataset_name, dataset_id)\u001b[0m\n\u001b[0;32m   2258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reference_dataset_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2259\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m reference_dataset_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2260\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2261\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly one of reference_dataset_id or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2262\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m reference_dataset_name may be given\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2263\u001b[0m         )\n\u001b[0;32m   2264\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreference_dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m reference_dataset_id\n\u001b[0;32m   2265\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m reference_dataset_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Haide\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langsmith\\client.py:754\u001b[0m, in \u001b[0;36m_get_with_retries\u001b[1;34m(self, path, params)\u001b[0m\n\u001b[0;32m    751\u001b[0m to_ignore_: Tuple[Type[\u001b[38;5;167;01mBaseException\u001b[39;00m], \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m*\u001b[39m(to_ignore \u001b[38;5;129;01mor\u001b[39;00m ()),)\n\u001b[0;32m    752\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 754\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(stop_after_attempt):\n\u001b[0;32m    755\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    756\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Haide\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langsmith\\client.py:706\u001b[0m, in \u001b[0;36mrequest_with_retries\u001b[1;34m(self, request_method, url, request_kwargs, stop_after_attempt, retry_on, to_ignore, handle_response)\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest_with_retries\u001b[39m(\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;241m/\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    685\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    686\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m requests\u001b[38;5;241m.\u001b[39mResponse:\n\u001b[0;32m    687\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Send a request with retries.\u001b[39;00m\n\u001b[0;32m    688\u001b[0m \n\u001b[0;32m    689\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;124;03m    ----------\u001b[39;00m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;124;03m    request_method : str\u001b[39;00m\n\u001b[0;32m    692\u001b[0m \u001b[38;5;124;03m        The HTTP request method.\u001b[39;00m\n\u001b[0;32m    693\u001b[0m \u001b[38;5;124;03m    pathname : str\u001b[39;00m\n\u001b[0;32m    694\u001b[0m \u001b[38;5;124;03m        The pathname of the request URL. Will be appended to the API URL.\u001b[39;00m\n\u001b[0;32m    695\u001b[0m \u001b[38;5;124;03m    request_kwargs : Mapping\u001b[39;00m\n\u001b[0;32m    696\u001b[0m \u001b[38;5;124;03m        Additional request parameters.\u001b[39;00m\n\u001b[0;32m    697\u001b[0m \u001b[38;5;124;03m    stop_after_attempt : int, default=1\u001b[39;00m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;124;03m        The number of attempts to make.\u001b[39;00m\n\u001b[0;32m    699\u001b[0m \u001b[38;5;124;03m    retry_on : Sequence[Type[BaseException]] or None, default=None\u001b[39;00m\n\u001b[0;32m    700\u001b[0m \u001b[38;5;124;03m        The exceptions to retry on. In addition to:\u001b[39;00m\n\u001b[0;32m    701\u001b[0m \u001b[38;5;124;03m        [LangSmithConnectionError, LangSmithAPIError].\u001b[39;00m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;124;03m    to_ignore : Sequence[Type[BaseException]] or None, default=None\u001b[39;00m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;124;03m        The exceptions to ignore / pass on.\u001b[39;00m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;124;03m    handle_response : Callable[[requests.Response, int], Any] or None, default=None\u001b[39;00m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;124;03m        A function to handle the response and return whether to continue\u001b[39;00m\n\u001b[1;32m--> 706\u001b[0m \u001b[38;5;124;03m        retrying.\u001b[39;00m\n\u001b[0;32m    707\u001b[0m \u001b[38;5;124;03m    **kwargs : Any\u001b[39;00m\n\u001b[0;32m    708\u001b[0m \u001b[38;5;124;03m        Additional keyword arguments to pass to the request.\u001b[39;00m\n\u001b[0;32m    709\u001b[0m \n\u001b[0;32m    710\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[0;32m    711\u001b[0m \u001b[38;5;124;03m    -------\u001b[39;00m\n\u001b[0;32m    712\u001b[0m \u001b[38;5;124;03m    Response\u001b[39;00m\n\u001b[0;32m    713\u001b[0m \u001b[38;5;124;03m        The response object.\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \n\u001b[0;32m    715\u001b[0m \u001b[38;5;124;03m    Raises:\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[38;5;124;03m    ------\u001b[39;00m\n\u001b[0;32m    717\u001b[0m \u001b[38;5;124;03m    LangSmithAPIError\u001b[39;00m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;124;03m        If a server error occurs.\u001b[39;00m\n\u001b[0;32m    719\u001b[0m \u001b[38;5;124;03m    LangSmithUserError\u001b[39;00m\n\u001b[0;32m    720\u001b[0m \u001b[38;5;124;03m        If the request fails.\u001b[39;00m\n\u001b[0;32m    721\u001b[0m \u001b[38;5;124;03m    LangSmithConnectionError\u001b[39;00m\n\u001b[0;32m    722\u001b[0m \u001b[38;5;124;03m        If a connection error occurs.\u001b[39;00m\n\u001b[0;32m    723\u001b[0m \u001b[38;5;124;03m    LangSmithError\u001b[39;00m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;124;03m        If the request fails.\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    726\u001b[0m     request_kwargs \u001b[38;5;241m=\u001b[39m request_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m    727\u001b[0m     request_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    728\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m    729\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_headers,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    735\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    736\u001b[0m     }\n",
      "\u001b[1;31mLangSmithError\u001b[0m: Failed to get https://api.smith.langchain.com/datasets in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/datasets?limit=1&name=System+Prompts', '{\"detail\":\"Forbidden\"}')"
     ]
    }
   ],
   "source": [
    "import langsmith\n",
    "from langchain import chat_models, smith\n",
    "from langchain import prompts\n",
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "\n",
    "# Define your runnable or chain below.\n",
    "prompt = prompts.ChatPromptTemplate.from_messages(\n",
    "  [\n",
    "    (\"system\", \"You are a helpful AI assistant.\"),\n",
    "    (\"human\", \"{your_input_key}\")\n",
    "  ]\n",
    ")\n",
    "llm = chat_models.ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Define the evaluators to apply\n",
    "eval_config = smith.RunEvalConfig(\n",
    "    evaluators=[\n",
    "        smith.RunEvalConfig.Criteria(\"helpfulness\"),\n",
    "        smith.RunEvalConfig.Criteria(\"controversiality\")\n",
    "    ],\n",
    "    custom_evaluators=[],\n",
    "    eval_llm=chat_models.ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    ")\n",
    "\n",
    "client = langsmith.Client()\n",
    "chain_results = client.run_on_dataset(\n",
    "    dataset_name=\"System Prompts\",\n",
    "    llm_or_chain_factory=chain,\n",
    "    evaluation=eval_config,\n",
    "    project_name=\"ATS resume and job matching\",\n",
    "    concurrency_level=5,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to batch ingest runs: LangSmithError('Failed to post https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from langsmith.run_trees import RunTree\n",
    "# This can be a user input to your app\n",
    "question = \"Can you summarize this morning's meetings?\"\n",
    "# Create a top-level run\n",
    "pipeline = RunTree(\n",
    "    name=\"Chat Pipeline\",\n",
    "    run_type=\"chain\",\n",
    "    inputs={\"question\": question}\n",
    ")\n",
    "# This can be retrieved in a retrieval step\n",
    "context = \"During this morning's meeting, we solved all world conflict.\"\n",
    "messages = [\n",
    "    { \"role\": \"system\", \"content\": \"You are a helpful assistant. Please respond to the user's request only based on the given context.\" },\n",
    "    { \"role\": \"user\", \"content\": f\"Question: {question}\\nContext: {context}\"}\n",
    "]\n",
    "# Create a child run\n",
    "child_llm_run = pipeline.create_child(\n",
    "    name=\"OpenAI Call\",\n",
    "    run_type=\"llm\",\n",
    "    inputs={\"messages\": messages},\n",
    ")\n",
    "# Generate a completion\n",
    "client = openai.Client()\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\", messages=messages\n",
    ")\n",
    "# End the runs and log them\n",
    "child_llm_run.end(outputs=chat_completion)\n",
    "child_llm_run.post()\n",
    "pipeline.end(outputs={\"answer\": chat_completion.choices[0].message.content})\n",
    "pipeline.post()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This chain was only tested with GPT-4. Performance may be significantly worse with other models.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "evaluate() missing 1 required positional argument: 'target'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 26\u001b[0m\n\u001b[0;32m     14\u001b[0m score_evaluator \u001b[38;5;241m=\u001b[39m LangChainStringEvaluator(\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore_string\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     16\u001b[0m     config\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m     }  \n\u001b[0;32m     23\u001b[0m )\n\u001b[0;32m     25\u001b[0m client \u001b[38;5;241m=\u001b[39m Client(api_key\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLANGCHAIN_API\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m---> 26\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m<dataset_name>\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcriteria_evaluator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_evaluator\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrevision_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthe version of your pipeline you are testing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: evaluate() missing 1 required positional argument: 'target'"
     ]
    }
   ],
   "source": [
    "from langsmith import Client  \n",
    "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
    "import os\n",
    "\n",
    "criteria_evaluator = LangChainStringEvaluator(\n",
    "    \"criteria\",\n",
    "    config={\n",
    "        \"criteria\": {\n",
    "            \"creativity\": \"Is this submission creative, imaginative, or novel?\",\n",
    "            \"conciseness\": \"Is this response concise and to the point?\",\n",
    "        }\n",
    "    }\n",
    ")\n",
    "score_evaluator = LangChainStringEvaluator(\n",
    "    \"score_string\",\n",
    "    config={\n",
    "        \"criteria\": {\n",
    "            \"accuracy\": \"How accurate is this prediction on a scale of 1-10?\" \n",
    "        },\n",
    "        # If you want the score to be saved on a scale from 0 to 1\n",
    "        \"normalize_by\": 10,\n",
    "    }  \n",
    ")\n",
    "\n",
    "client = Client(api_key=os.getenv(\"LANGCHAIN_API\"))\n",
    "evaluate(\n",
    "    data=\"<dataset_name>\",\n",
    "    evaluators=[\n",
    "        criteria_evaluator,\n",
    "        score_evaluator\n",
    "    ], \n",
    "    metadata={\"revision_id\": \"the version of your pipeline you are testing\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7589c6633c4f415ca3f781db8875d44a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32011f99db784516a36190a0272b3be2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llmlingua import PromptCompressor\n",
    "\n",
    "prompt = \"\"\" You are an ATS (Applicant Tracking System) scanner which is designed to scan resume {0} for specific keywords and phrases. \n",
    "        Your task is to evaluate the resume against the provided job description {1}. Rate the resume between 1 - 10 where 1 being the lowest and 10 being the highest match with the job description. \n",
    "        The output should come as the rating / highest score, explain the reason for the score, missing keywords, lastly final thoughts with the list of best resources with links  to gain missing skills better if from the hiring company.\n",
    "        \"\"\"\n",
    "llm_lingua = PromptCompressor(\"openai-community/gpt2\", device_map=\"cpu\")\n",
    "compressed_prompt = llm_lingua.compress_prompt(prompt, instruction=\"\", question=\"\", target_token=200)\n",
    "\n",
    "# > {'compressed_prompt': 'Question: Sam bought a dozen boxes, each with 30 highlighter pens inside, for $10 each box. He reanged five of boxes into packages of sixlters each and sold them $3 per. He sold the rest theters separately at the of three pens $2. How much did make in total, dollars?\\nLets think step step\\nSam bought 1 boxes x00 oflters.\\nHe bought 12 * 300ters in total\\nSam then took 5 boxes 6ters0ters.\\nHe sold these boxes for 5 *5\\nAfterelling these  boxes there were 3030 highlighters remaining.\\nThese form 330 / 3 = 110 groups of three pens.\\nHe sold each of these groups for $2 each, so made 110 * 2 = $220 from them.\\nIn total, then, he earned $220 + $15 = $235.\\nSince his original cost was $120, he earned $235 - $120 = $115 in profit.\\nThe answer is 115',\n",
    "#  'origin_tokens': 2365,\n",
    "#  'compressed_tokens': 211,\n",
    "#  'ratio': '11.2x',\n",
    "#  'saving': ', Saving $0.1 in GPT-4.'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_prompt['compressed_prompt'] == prompt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
